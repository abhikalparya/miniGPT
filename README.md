# Mini GPT Language Model

![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)
![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=for-the-badge&logo=jupyter&logoColor=white)

This project implements a mini version of a Generative Pre-trained Transformer (GPT) language model. It utilizes the decoder part of the Transformer model and self-attention mechanism to generate text predictions.

## Overview

The model is trained on a provided text dataset and can generate text sequences based on the learned patterns. It's implemented in PyTorch and designed to be lightweight for educational purposes.

## Architecture

![miniGPT](https://github.com/abhikalparya/miniGPT/assets/81465377/ae85aba0-69b3-465c-b268-da6da30e229e)

The `green part` is the architecture of Mini GPT Language Model which is inspired from the traditional architecture of transformers.

## Dependencies

- Python 3.x
- PyTorch

